import os
import numpy as np
import glob
import pickle
import json
import math
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.decomposition import IncrementalPCA
from sklearn.utils import shuffle
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Seeds
np.random.seed(42)
tf.random.set_seed(42)

# Pastas
data_folder = "dados_teste_3_dias"
quant_folder = "modelo_final_quantizado"
os.makedirs(quant_folder, exist_ok=True)

# Par√¢metros
n_components = 50
batch_size_pca = 64
batch_size_train = 64

# Coleta de arquivos e shuffle
arquivos_csv = sorted(glob.glob(f"{data_folder}/*.csv"))
arquivos_csv, _ = shuffle(arquivos_csv, arquivos_csv, random_state=42)

# PCA incremental
print(f"\nüîç {len(arquivos_csv)} arquivos encontrados.")
print("üöÄ Ajustando PCA incrementalmente...")
pca = IncrementalPCA(n_components=n_components)

for i in tqdm(range(0, len(arquivos_csv), batch_size_pca), desc="PCA Fit"):
    batch = []
    for path in arquivos_csv[i:i+batch_size_pca]:
        arr = np.nan_to_num(np.loadtxt(path, delimiter=","), nan=0.0)
        batch.append(arr.flatten())
    batch_arr = np.vstack(batch).astype("float32")
    pca.partial_fit(batch_arr)

# Transforma√ß√£o
print("\nüîç Transformando dados com PCA...")
X_pca = []
y_full = []

for path in tqdm(arquivos_csv, desc="PCA Transform"):
    arr = np.nan_to_num(np.loadtxt(path, delimiter=","), nan=0.0)
    X_pca.append(pca.transform(arr.flatten().reshape(1, -1))[0])
    y_full.append(float(os.path.basename(path).split("_")[-1].replace(".csv", "")))

X_pca = np.array(X_pca, dtype="float32")
y_full = np.array(y_full, dtype="float32")

print(f"‚úÖ X_pca shape: {X_pca.shape}")
print(f"‚úÖ y_full shape: {y_full.shape}")

# Modelo com BatchNorm e callbacks
model = Sequential([
    Dense(128, activation="relu", input_shape=(n_components,)),
    BatchNormalization(),
    Dropout(0.3),
    Dense(64, activation="relu"),
    BatchNormalization(),
    Dropout(0.3),
    Dense(1)
])

model.compile(optimizer=Adam(1e-3), loss="mae", metrics=["mae"])

callbacks = [
    EarlyStopping(monitor="val_loss", patience=20, restore_best_weights=True),
    ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=10)
]

# Treinamento
print("\nüèãÔ∏è Treinando modelo...")
hist = model.fit(
    X_pca, y_full,
    validation_split=0.2,
    epochs=400,
    batch_size=batch_size_train,
    shuffle=True,
    callbacks=callbacks,
    verbose=2
)

# Salvar modelo original
model.save(os.path.join(quant_folder, "modelo_final.keras"))

# Quantiza√ß√£o seletiva - somente Dense
from tensorflow_model_optimization.quantization.keras import quantize_annotate_layer, quantize_apply

print("\nüîß Anotando camadas Dense para quantiza√ß√£o...")

def quantize_if_dense(layer):
    if isinstance(layer, tf.keras.layers.Dense):
        return quantize_annotate_layer(layer)
    return layer

annotated_model = tf.keras.models.clone_model(
    model,
    clone_function=quantize_if_dense
)

quantized_model = quantize_apply(annotated_model)
quantized_model.compile(optimizer=Adam(1e-3), loss="mae", metrics=["mae"])
quantized_model.save(os.path.join(quant_folder, "modelo_final_quantizado.keras"))

# Fun√ß√£o correta de representative_dataset
def representative_dataset():
    for i in range(min(200, len(X_pca))):
        input_data = np.expand_dims(X_pca[i].astype(np.float32), axis=0)
        yield [input_data]

# Convers√£o para TFLite
print("\nüì¶ Convertendo para TFLite...")
converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
tflite_model = converter.convert()

with open(os.path.join(quant_folder, "modelo_quant.tflite"), "wb") as f:
    f.write(tflite_model)

# M√©tricas
y_pred = model.predict(X_pca).flatten()
mae = mean_absolute_error(y_full, y_pred)
rmse = math.sqrt(mean_squared_error(y_full, y_pred))
r2 = r2_score(y_full, y_pred)

with open(os.path.join(quant_folder, "metricas.txt"), "w") as f:
    f.write(f"MAE: {mae:.2f} kWh\n")
    f.write(f"RMSE: {rmse:.2f} kWh\n")
    f.write(f"R2: {r2:.4f}\n")

# Hist√≥rico serializ√°vel
history_clean = {k: [float(x) for x in v] for k, v in hist.history.items()}
with open(os.path.join(quant_folder, "training_history.json"), "w") as f:
    json.dump(history_clean, f, indent=2)

# Gr√°fico com unidades
def plot_curves(hist, folder):
    epochs = range(1, len(hist["loss"]) + 1)
    plt.figure(figsize=(8,6))
    plt.plot(epochs, hist["loss"], label="Train MAE (kWh)")
    plt.plot(epochs, hist["val_loss"], label="Val MAE (kWh)")
    plt.xlabel("Epoch")
    plt.ylabel("MAE (kWh)")
    plt.title("Training & Validation MAE")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(folder, "training_validation_mae_kwh.png"))
    plt.close()

plot_curves(history_clean, quant_folder)

print("\n‚úÖ Tudo pronto! Gr√°ficos sem offset e com unidades.")
